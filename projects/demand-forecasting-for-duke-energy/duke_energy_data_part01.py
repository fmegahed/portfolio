# -*- coding: utf-8 -*-
"""duke_energy_data_part01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ji9MjlSo93nHXWxbz3tHWJKApDODRYf

# Objectives for Today

- Pull Duke Energy Data from 2020-2025 and save it to a CSV/ a single data frame.  

- Identify simple "external" variables that can help improve the forecasting of this data

# Libraries
"""

!pip install statsforecast utilsforecast coreforecast mlforecast neuralforecast meteostat

import pandas as pd
import holidays
from datetime import datetime

from meteostat import Point, Daily

from utilsforecast.plotting import plot_series

from statsforecast import StatsForecast
from statsforecast.models import Naive, SeasonalNaive, AutoETS, AutoARIMA

"""# Extracting the Data"""

url = "https://www.duke-energyohiocbp.com/Portals/0/Documents/Actual_Hourly_Loads_by_Class_2025.xlsx"

df = (
    pd.read_excel(io = url, sheet_name=0, header = 1)
    [['REPORT DAY' , 'HOUR ENDING', 'TOTAL']]
    .rename(columns = {'REPORT DAY' : 'ds', 'HOUR ENDING' : 'hour_ending', 'TOTAL' : 'y'})
    .assign(unique_id = 'ohio_energy')
    # optional if you want to have the following order for the columns
    [['unique_id', 'ds', 'hour_ending', 'y']]
)

df.info()

"""## Our Custom Function for Getting Duke Energy Data

All the data are available on [Duke's website](https://www.duke-energyohiocbp.com/Documents/LoadandOtherData.aspx)
"""

def get_duke_energy_data(year):
    """
    Our custom function to get Duke Energy data for a given year.
    """
    if year not in [2022, 2023]:
      url = f"https://www.duke-energyohiocbp.com/Portals/0/Documents/Actual_Hourly_Loads_by_Class_{year}.xlsx"
    elif year == 2022:
      url = "https://www.duke-energyohiocbp.com/Portals/0/Documents/Actual_Hourly_Loads_by_Class_2022_20230307.xlsx"
    elif year == 2023:
      url = "https://www.duke-energyohiocbp.com/Portals/0/Documents/Actual_Hourly_Loads_by_Class_2023_20240507.xlsx"

    df = (
        pd.read_excel(io = url, sheet_name=0, header = 1)
        [['REPORT DAY' , 'HOUR ENDING', 'TOTAL']]
        .rename(columns = {'REPORT DAY' : 'ds', 'HOUR ENDING' : 'hour_ending', 'TOTAL' : 'y'})
        .assign(unique_id = 'ohio_energy')
        # optional if you want to have the following order for the columns
        [['unique_id', 'ds', 'hour_ending', 'y']]
    )

    return df

years = range(2020, 2026)

duke_df = pd.concat([get_duke_energy_data(year) for year in years])

display(duke_df)

"""# Reduce the Data to Max Load by Day

We are doing this for two reasons: (a) reduces the size of the data (which makes it more managable), and (b) academic: we want to give some baseline methods a chance instead of dealing with potentially multiple seasonality.
"""

duke_daily_df = (
    duke_df.copy()
    .groupby(['unique_id', 'ds'])
    .agg({'y' : 'max'})
    .reset_index()
)

display(duke_daily_df)

duke_daily_df.to_csv('duke_daily_df.csv', index = False)

"""## Visualize the Data"""

plot_series(
    df = duke_daily_df,
    engine = 'plotly'
)

"""## Extracting Holidays"""

us_holidays = holidays.US(years = range(2020, 2026))
us_holidays

duke_daily_df_holidays = (
    duke_daily_df.copy()
    .assign(
        day_of_week = lambda x: x['ds'].dt.day_name(),
        month = lambda x: x['ds'].dt.month_name(),
        holiday_name = lambda x: x['ds'].apply(lambda y: us_holidays.get(y, None))
    )
)

display(duke_daily_df_holidays)

"""## Extracting Weather for Cincy"""

cincy_weather_station = Point(39.103119, -84.512016, 167)

start = datetime(2020, 1, 1)
# cheating a bit: using data past our duke energy data to use in forecasting future data
# in the real world, you will have to forecast these values
end = datetime(2025, 10, 31)

# Get daily data for Cincy
cincy_data = Daily(cincy_weather_station, start, end)
cincy_data = cincy_data.fetch()

cincy_data = (
    cincy_data.reset_index()
    .rename(columns = {'time' : 'ds'})
    [['ds', 'tavg', 'prcp', 'snow', 'tmin', 'tmax']]
    # rename these columns to have _cincy as their suffix
    .rename(columns = {col : col + '_cincy' for col in cincy_data.columns if col not in ['ds']})
)

display(cincy_data)


# Get weather station for cleveland
cleveland_weather_station = Point(41.505493, -81.681290, 200)

# Get daily data for Cleveland
cle_data = Daily(cleveland_weather_station, start, end)
cle_data = cle_data.fetch()

cle_data = (
    cle_data.reset_index()
    .rename(columns = {'time' : 'ds'})
    [['ds', 'tavg', 'prcp', 'snow', 'tmin', 'tmax']]
    # rename these columns to have _cle as their suffix
    .rename(columns = {col : col + '_cle' for col in cle_data.columns if col not in ['ds']})
)

display(cle_data)

"""# Merging the Three Datasets together

Technically, we have:  

- `duke_daily_data_holidays`: This already concatenated 6 different excel files from duke's website (data from 2020 up to and including 2025). We aggregated that data by max consumption in a given day. We also added to this **three** potential predictors: (a) `day_of_the_week`, (b) `month`, and (c) `holiday_name`.  

- `cincy_data`: Has for every `ds`, a bunch of weather related variables for cincy. **Note** we downselected those variables by using `[[]]`.

- `cle_data`: Has for every `ds`, a bunch of weather related variables for cle.

We had the option to either make the size of our merged data to include date stamps up to Oct 31, 2025 (last date we pulled weather data for) or the shorter date for July 31, 2025 (last date we have Duke data for - as of our class on Nov 6, 2025). As a class, we decided to go the shorter route. Both approaches have pros and cons.
"""

# we technically had an object called df in this file
# we do not mind overwriting it as we did only to guide us in writing our custom function

df = (
   duke_daily_df_holidays
   .merge(
       cincy_data,
       on = 'ds',
       how = 'left' # num_rows defined by duke_daily_df_holidays
   )
   .merge(
       cle_data,
       on = 'ds',
       how = 'left' # num_rows defined by duke_daily_df_holidays
   )
   .assign(
       holiday_name = lambda x: x['holiday_name'].fillna('Not a Holiday'),
   )
)

display(df)

df.to_csv('merged_duke_data.csv', index = False)

"""# Cross Validation to Compare Different Models

For now, we will focus on the [statsforecast](https://nixtlaverse.nixtla.io/statsforecast/index.html).

The models we will run will be:  
  + *Baseline Models:* `Naive`, `SeasonalNaive`, `AutoETS()` [automates the exponential smoothing family of models and in general ETS type models by optimizing the model parameters based on the AICc information criterion; see [AutoETS](https://nixtlaverse.nixtla.io/statsforecast/src/core/models.html#autoets-2)].  
    * **Note:** None of the models above can be used with any predictors, which means that when we pass these list of models into the `.cross_validation()`, we will have to `df = df[['unique_id', 'ds', 'y']]`.
  + *Baseline ARIMA with no predictors:* Similar to `AutoETS()`, we will use the `AutoARIMA()` version. This will select the best `p, d, q` combinations for the model, and it can also allow for fitting a seasonal ARIMA model with `P, D, Q`. **Note** This can be ran with and without predictors so we will have a different instance for the model with predictors.
"""

baseline_models_no_preds = [
    Naive(alias = 'Naive'),
    SeasonalNaive(season_length=7, alias = 'SeasonalNaive'),
    AutoETS(season_length=7, alias = 'AutoETS'),
    AutoARIMA(season_length=7, alias = 'AutoARIMA_NoPred')
]

sf_base_no_preds = StatsForecast(
    models=baseline_models_no_preds,
    freq='D',
    n_jobs=-1
)

cv_base_no_preds = sf_base_no_preds.cross_validation(
    h = 28,
    df = df[['unique_id', 'ds', 'y']],
    n_windows=12, # to essentially capture all seasons
    step_size=28
)

display(cv_base_no_preds)

baseline_models_w_preds = [
    AutoARIMA(season_length=7, alias = 'AutoARIMA_WPred')
]

sf_base_w_preds = StatsForecast(
    models=baseline_models_w_preds,
    freq='D',
    n_jobs=-1
)

cv_base_w_preds = sf_base_w_preds.cross_validation(
    h = 28,
    df = (
        df
        .assign(
            # convert string columns day_of_week	month	holiday_name to cat
            day_of_week = lambda x: x['day_of_week'].astype('category'),
            month = lambda x: x['month'].astype('category'),
            holiday_name = lambda x: x['holiday_name'].astype('category')
        )
        ),
    n_windows=12, # to essentially capture all seasons
    step_size=28
)

display(cv_base_w_preds)

from utilsforecast.losses import mape, rmse
from utilsforecast.evaluation import evaluate

eval_base_no_preds = evaluate(
    df = cv_base_no_preds,
    metrics= [mape, rmse],
    models = ['Naive', 'SeasonalNaive', 'AutoETS', 'AutoARIMA_NoPred']
)

display(eval_base_no_preds)

eval_base_w_preds = evaluate(
    df = cv_base_w_preds,
    metrics= [mape, rmse],
    models = ['AutoARIMA_WPred']
)

display(eval_base_w_preds)